# 哈夫曼编码

## 文件存储

        文件是将数据存储在磁盘等存储媒介的一种形式。程序文件中最基本的存储数据单位是`字节`。文件的大小不管是 xxxKB、xxxMB等来表示，都是因为文件是以字节 `B = Byte` 为单位来存储的。

        文件就是字节数据的集合。用 1 字节（8 位）表示的字节数据有 256 种，用二进制表示的话就是 0000 0000 - 1111 1111 。如果文件中存储的数据是文字，那么该文件就是文本文件。如果是图形，那么该文件就是图像文件。在任何情况下，文件中的字节数都是`连续存储`的。

![image.png](https://segmentfault.com/img/bVbzWXo)

## 文件压缩

> 为什么需要文件压缩

- **节省空间**：比如一张10MB的照片压缩后可能变成2MB，节省硬盘/云存储空间。
- **加快传输**：小文件上传下载更快，比如微信发图片、视频会议实时传输。
- **降低成本**：云存储按容量收费，压缩能直接减少费用。

### Run-Length Encoding (游程编码）

        首先让我们来尝试对 `AAAAAABBCDDEEEEEF` 这 17 个半角字符的文件（文本文件）进行压缩。虽然这些文字没有什么实际意义，但是很适合用来描述 `RLE` 的压缩机制。

由于半角字符（其实就是英文字符）是作为 1 个字节保存在文件中的，所以上述的文件的大小就是 17 字节。如图

<img title="" src="file:///C:/Users/Administrator/AppData/Roaming/marktext/images/2025-04-29-15-10-19-image.png" alt="" data-align="center" width="301">

        那么，如何才能压缩该文件呢？大家不妨也考虑一下，只要是能够使文件小于 17 字节，我们可以使用任何压缩算法。

        最显而易见的一种压缩方式我觉得你已经想到了，就是把相同的字符`去重化`，也就是 `字符 * 重复次数` 的方式进行压缩。所以上面文件压缩后就会变成下面这样

<img title="" src="file:///C:/Users/Administrator/AppData/Roaming/marktext/images/2025-04-29-15-13-07-image.png" alt="" width="389" data-align="center">

        从图中我们可以看出，**AAAAAABBCDDEEEEEF** 的17个字符成功被压缩成了 **A6B2C1D2E5F1** 的12个字符，也就是 12 / 17 = 70%，压缩比为 70%，压缩成功了。

        像这样，把文件内容用 `数据 * 重复次数` 的形式来表示的压缩方法成为 `RLE(Run Length Encoding, 行程长度编码)` 算法。

### 文件压缩的“原理”

#### 核心思想：消除冗余   更紧凑的表示方法

- **数据冗余**：文件中存在大量重复或可以预测的内容。
- **压缩的本质**：找到数据的规律，用更紧凑的方式表示它。

#### RLE 算法的缺点

        RLE 的压缩机制比较简单，所以 RLE 算法的程序也比较容易编写，所以使用 RLE 的这种方式更能让你体会到压缩思想，但是 RLE 只针对特定序列的数据管用，下面是 RLE 算法压缩汇总

| 文件类型  | 压缩前文件大小 | 压缩后文件大小 | 压缩比率 |
| ----- |:-------:| ------- | ---- |
| 文本文件  | 14862字节 | 29065字节 | 199% |
| 图像文件  | 96062字节 | 38328字节 | 40%  |
| EXE文件 | 24576字节 | 15198字节 | 62%  |

        通过上表可以看出，使用 RLE 对文本文件进行压缩后的数据不但没有减小反而增大了！几乎是压缩前的两倍！因为文本字符中连续的字符并不多见。

        就像上面我们探讨的这样，RLE 算法只针对`连续`的字节序列压缩效果比较好，假如有一连串不相同的字符该怎么压缩呢？比如说`ABCDEFGHIJKLMNOPQRSTUVWXYZ`，26个英文字母所占空间应该是 26 个字节，我们用 RLE 压缩算法压缩后的结果为 `A1B1C1D1E1F1G1H1I1J1K1L1M1N1O1P1Q1R1S1T1U1V1W1X1Y1Z1` ，占用 52 个字节，压缩完成后的容量没有减少反而增大了！这显然不是我们想要的结果，所以这种情况下就不能再使用 RLE 进行压缩。

## 信息熵

        信息熵（Entropy）是信息论中的一个核心概念，**用来衡量信息的不确定性或平均信息量**。它由克劳德·香农（Claude Shannon）在 1948 年的信息理论中提出，因此也称为 **香农熵**（Shannon Entropy）。

      

![](C:\Users\Administrator\AppData\Roaming\marktext\images\2025-04-29-16-15-13-image.png)

![](C:\Users\Administrator\AppData\Roaming\marktext\images\2025-04-29-16-16-49-image.png)

信息熵的直观理解

1. 如果一个系统的所有状态都等可能发生（比如抛一枚公平的硬币），则信息熵最大，因为我们无法预先确定它的结果。

2. 如果某个状态的概率非常高（比如天气预报说 99% 明天是晴天），那么信息熵会很低，因为我们几乎可以确定它的结果。

3. 如果一个事件是确定的（概率是 1），那么信息熵为 0，因为没有不确定性，不需要额外的信息来描述它。

      信息熵反映了一个消息源的平均信息量，如果某个系统的**不确定性越大**，则它包含的**信息量也越大，信息熵也就越高**。相反，如果某个系统的**确定性越高**（比如某个事件总是发生），那么它的**信息熵就越低**。

## 哈夫曼编码

        **哈夫曼编码（Huffman Coding）** 是1952年由大卫·哈夫曼（David Huffman）提出的一种**无损数据压缩算法**。它的核心思想是通过**变长编码**，**为高频字符分配短编码，低频字符分配长编码**，从而减少数据的总体存储或传输体积。

- **变长编码**：不同字符的编码长度不同（与固定长度的ASCII码相反）。
- **频率统计**：字符出现频率越高，编码越短，频率越低，编码越长。

### 哈夫曼树（最优二叉树）

哈夫曼编码通过构建一棵**二叉树**来分配编码，树的结构遵循以下规则：

- **叶子节点**：表示字符及其频率。
- **非叶子节点**：表示合并后的权重（子节点频率之和）。
- **路径规则**：向左走为`0`，向右走为`1`，从根到叶子的路径即字符的编码。

#### 哈夫曼树的构建过程如下：

1. **统计频率**: 统计每个字符在数据中出现的频率。

2. **构建优先队列**: 将每个字符及其频率作为一个节点， 并按照频率从小到大的顺序放入优先队列（最小堆）。

3. **构建哈夫曼树**:
   
   a. 从优先队列中取出两个频率最小的节点， 构建一个新的父节点， 其频率为两个子节点频率之和。
   
   b. 将新的父节点插回优先队列。
   
   重复上述步骤， 直到队列中只剩下一个节点， 即哈夫曼树的根节点。

4. **生成编码**: 从根节点出发， 对每个节点规定左边路径为0， 右边路径为1， 最终从根节点到叶节点的路径构成了该字符的哈夫曼编码。

#### 构建示例:

**对字符串 hhhhhhhhgggggggffffffeeeeeddddcccbba 进行哈夫曼编码。**

1. 统计频率
   
   各个字符出现的频率为: h:8，g:7，f:6，e:5，d:4，c:3，b:2，a:1    

2. 构建优先队列
   
   **出现频率低的排在最前面，则该队列的初始状态为:**
   
   ![](C:\Users\Administrator\AppData\Roaming\marktext\images\2025-04-30-10-49-25-image.png)

3. 构建哈弗曼树
   
   **取出前两个节点， 频率相加作为两个节点的父节点， 然后将该节点放入优先队列， 重复上述步骤，直到队列中只剩下一个节点:**
   
   ![](C:\Users\Administrator\AppData\Roaming\marktext\images\2025-04-30-10-50-17-image.png)

![](C:\Users\Administrator\AppData\Roaming\marktext\images\2025-04-30-10-50-39-image.png)

![](C:\Users\Administrator\AppData\Roaming\marktext\images\2025-04-30-10-50-49-image.png)

![](C:\Users\Administrator\AppData\Roaming\marktext\images\2025-04-30-10-54-58-image.png)

![](C:\Users\Administrator\AppData\Roaming\marktext\images\2025-04-30-10-55-10-image.png)

![](C:\Users\Administrator\AppData\Roaming\marktext\images\2025-04-30-10-56-23-image.png)

![](C:\Users\Administrator\AppData\Roaming\marktext\images\2025-04-30-10-57-10-image.png)

**最终的需要编码的所有字符都在叶子节点上。**

> 注: 如果新节点的数字在队列中存在，比如数字1+2生产新的节点3，则插入后 新节点3 排在已有3的前面和后面都可以，不影响哈夫曼编码，只是要注意解码时要用同一套编码字典即可。

4. 生成编码
   
   从根节点出发，每个叶子节点所经过的路径即为哈夫曼编码，经过左边路径+0，经过右边路径+1

![](C:\Users\Administrator\AppData\Roaming\marktext\images\2025-04-30-12-02-54-image.png)

**所以最终的编码为:**

```java
a:11000，b:11001，c:1101，d:100，e:101，f:111，g:00，h:01
```

我们可以观察到，**频率越高的字符，经过的路径越短，则编码越短**。

### 信息熵：压缩的理论极限

信息熵是数据压缩的理论极限，这是因为它代表了在不丢失信息的情况下，对数据进行编码所需的最小平均比特数。下面我通过几个例子来解释这一概念。

#### 实例说明

**例子1：不同概率分布的字符编码**

假设有一个文本，只包含四个字符A、B、C、D，它们出现的概率分别是：

- A: 1/2 (50%)
- B: 1/4 (25%)
- C: 1/8 (12.5%)
- D: 1/8 (12.5%)

这个信息源的熵为： H = -(1/2)log₂(1/2) - (1/4)log₂(1/4) - (1/8)log₂(1/8) - (1/8)log₂(1/8) = 0.5 + 0.5 + 0.375 + 0.375 = 1.75比特/符号

这意味着理论上每个字符平均至少需要1.75比特来编码。

使用霍夫曼编码，我们可以得到：

- A: 0 (1位)
- B: 10 (2位)
- C: 110 (3位)
- D: 111 (3位)

平均编码长度 = 1×(1/2) + 2×(1/4) + 3×(1/8) + 3×(1/8) = 1.75比特/符号

这正好达到了熵的理论极限。

**例子2：均匀分布与非均匀分布**

考虑两种情况：CBAD

1. 均匀分布：四个字符A、B、C、D各占25%
   
   - 熵 = -4×(1/4)log₂(1/4) = 2比特/符号
   - 无法压缩，每个字符都需要2比特

2. 极度不均匀分布：A占97%，B、C、D各占1%
   
   - 熵约为0.24比特/符号
   - 可以设计编码使平均长度接近0.24比特，压缩效果显著

**例子3：极端情况**

如果一个信息源只产生一种符号（概率为1），其熵为0。这意味着不需要任何比特来编码，因为没有不确定性。

#### 为什么是极限？

1. **无损压缩定理**：香农的信源编码定理证明，对任何无损压缩方案，平均编码长度不可能小于信息熵。

2. **实际应用**：实际压缩算法如霍夫曼编码、算术编码等都试图接近这个理论极限。

3. **冗余与压缩**：信息熵越低，数据冗余度越高，可压缩性越强；反之，熵越高，数据越"随机"，压缩空间越小。

总之，信息熵代表了压缩的理论极限，因为它量化了信息的本质不确定性，而这种不确定性决定了我们至少需要多少比特来无损地表示这些信息。

**哈夫曼树的特点：**

- **前缀无冲突**：生成的编码不会有前缀重叠，确保解码时不会产生歧义。
- **最优编码**：在已知字符频率的情况下，哈夫曼树生成的编码是最短的。
- **贪心算法应用**：构建过程采用了**贪心策略**，每次合并最小的两个节点，保证了整体的最优解。

>  **前缀冲突（Prefix Collision）** 指的是在编码系统中，**某个编码恰好是另一个编码的前缀**（开头部分）。这会导致解码时出现歧义，无法确定一个编码应该被拆分成哪几个部分。



#### 为什么会出现前缀冲突？

- **变长编码**：在变长编码（比如哈夫曼编码）中，不同符号的编码长度不同。

- **前缀重叠**：如果短编码是长编码的前缀，解码器读到短编码时会提前“误判”，无法继续解析后续的编码。

#### 例子解析

假设我们要用二进制编码表示字母 **A、B、C**，设计出以下两种方案：

#### 方案1（有前缀冲突）：

- A → `0`

- B → `01`

- C → `11`

**问题**：A 的编码 `0` 是 B 的编码 `01` 的前缀。  
**后果**：当遇到 `01` 时，解码器会先读到 `0`，以为是字母 A，剩下的 `1` 无法单独匹配，导致错误。

#### 方案2（无前缀冲突）：

- A → `0`

- B → `10`

- C → `11`

**优点**：所有编码都不是其他编码的前缀，可以唯一解码。

**为什么哈夫曼编码能够保证是前缀编码?**

        因为需要编码的字符都在叶子节点上，所以某一个字符的编码不可能是其他字符编码的前缀，（字符都是叶子结点,根到一个字符不会路过另一个字符）这样我们面对一长串哈夫曼编码解码的时候，不会面临歧义。

### 哈夫曼树的优点

1. 高效的压缩性能
   哈夫曼树能够根据字符出现的频率分配变长编码，使得高频字符使用较短的编码，低频字符使用较长的编码，从而最大程度地减少平均编码长度。这种自适应的编码方式使哈夫曼编码在很多实际应用中都能达到接近最优的压缩效果。

2. 简单易实现
   哈夫曼编码的算法相对简单，只需进行频率统计、优先队列的操作以及树的构建。其实现不需要复杂的数学运算或数据结构，因此在实际应用中非常易于实现。

3. 解码效率高
   哈夫曼树的解码过程非常高效，因为它是一种前缀码，保证了每个编码不会是另一个编码的前缀。这意味着解码时无需回溯，只需从根节点开始，根据编码的比特顺序遍历哈夫曼树，直到到达叶节点即可完成一个字符的解码。

4. 无损压缩
   哈夫曼编码是一种无损压缩算法，保证了压缩后的数据可以完全还原成原始数据，没有任何信息丢失。因此，它特别适用于需要精确还原的数据类型，如文本文件和程序代码等。

5. 灵活性强
   哈夫曼树可以根据具体的应用需求进行调整。对于不同的输入数据，哈夫曼编码总是基于当前数据的统计特性进行编码，因此能够自适应不同的数据集，提供灵活的压缩方案。

6. 广泛的应用
   哈夫曼编码被广泛应用于各种压缩标准和工具中，例如ZIP文件压缩、JPEG图像压缩、MP3音频压缩等。其应用范围涵盖了从文本文件到多媒体文件的各种类型的数据。

7. 较低的存储和传输开销
   由于哈夫曼编码能够显著减少数据的冗余，因此在存储和传输数据时，可以有效减少所需的存储空间和传输带宽。这在大数据存储、网络传输等场景中尤为重要。

## 哈夫曼编码在现代压缩算法中的应用

哈夫曼编码（Huffman Coding）作为一种经典的无损压缩技术，虽然诞生于1952年，但在现代压缩算法中仍然扮演着重要角色。以下是哈夫曼编码在现代压缩领域的主要应用：

### 1. 文本和通用数据压缩

###### DEFLATE算法

- **ZIP/gzip/PNG**：这些广泛使用的压缩格式采用DEFLATE算法，该算法结合了LZ77字典编码和哈夫曼编码。哈夫曼编码负责对LZ77输出的距离-长度对和字面量进行二次压缩。
- **zlib库**：这个被广泛使用的压缩库也采用DEFLATE算法，其中哈夫曼编码是核心组件之一。

##### 其他通用压缩格式

- **bzip2**：虽然主要基于Burrows-Wheeler变换，但在其最终编码阶段使用了哈夫曼编码。
- **7-Zip的LZMA**：在某些实现中结合了哈夫曼编码来提高压缩效率。

### 2. 图像压缩

###### JPEG格式

JPEG是哈夫曼编码最著名的应用之一：

- 在量化后的DCT（离散余弦变换）系数编码中使用哈夫曼编码
- 对DC系数和AC系数分别使用不同的哈夫曼表
- 标准JPEG通常包含预定义的哈夫曼表，但也支持自适应哈夫曼表

###### 其他图像格式

- **TIFF**：可选择使用哈夫曼编码作为其压缩选项之一
- **GIF**：虽然主要使用LZW算法，但某些变种实现中结合了哈夫曼技术

### 3. 音频压缩

###### MP3和AAC

- 在这些流行的音频压缩格式中，哈夫曼编码用于编码频谱数据
- 特别是在MPEG-1 Layer III (MP3)中，哈夫曼编码是最终比特流生成的关键步骤

###### FLAC（无损音频压缩）

- 在其可变长度编码阶段使用了类似哈夫曼的编码技术

### 4. 视频压缩

###### H.264/AVC和H.265/HEVC

- 在这些现代视频编码标准中，哈夫曼编码被用于熵编码阶段
- H.264使用CAVLC（上下文自适应可变长度编码），这是一种基于哈夫曼原理的编码方式
- 虽然H.265主要使用CABAC（上下文自适应二进制算术编码），但在某些配置文件中仍保留了基于哈夫曼的编码选项

### 5. 现代哈夫曼编码变种

#### 自适应哈夫曼编码

- 在数据流处理过程中动态更新编码表，适用于特性未知的数据流
- 在某些网络传输协议和实时压缩应用中使用

#### 规范哈夫曼编码

- 确保相同频率的符号按照特定顺序分配编码，提高压缩一致性
- 在需要确定性压缩结果的应用中使用

#### 截断哈夫曼编码

- 限制最大编码长度，在某些硬件实现中很有用
- 在资源受限的嵌入式系统中应用广泛

### 6. 与现代技术的结合

#### 与机器学习的结合

- 使用机器学习预测符号概率分布，优化哈夫曼树构建
- 在自适应压缩系统中应用

#### 并行化实现

- 现代GPU和多核处理器上的并行哈夫曼编码实现
- 显著提高了编解码速度，适用于大规模数据处理

### 7. 哈夫曼编码的优势与局限

#### 优势

- 实现简单，计算复杂度低
- 解码速度快，适合实时应用
- 无需浮点运算，适合嵌入式系统

#### 局限

- 理论上不如算术编码接近熵极限
- 需要存储或传输编码表
- 对于高度偏斜的概率分布效率较低

## 结论

尽管有更新的压缩技术出现，哈夫曼编码因其简单性、效率和易于实现的特点，仍然在现代压缩算法中占有重要地位。它经常作为更复杂压缩系统的组件使用，或与其他技术结合以获得更好的压缩性能。在对速度和简单性要求高于极限压缩率的场景中，哈夫曼编码仍然是首选方案。

QA：

π可以被压缩吗？

π的“可压缩性”取决于表示方式。通过数学符号或生成算法，π可以被极致压缩；但若存储其展开的小数位，则无法有效压缩。
